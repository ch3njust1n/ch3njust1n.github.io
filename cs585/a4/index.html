<!DOCTYPE html>
<html>
<header>
    <meta charset="UTF-8">
    <base href="/">
    <title>Justin Chen - CS585</title>
    <link rel='stylesheet' type='text/css' href='css/normalize.css'>
    <link rel='stylesheet' type='text/css' href='css/style.css'>
    <style>
        html {
            font-family: Avenir Next;
        }
        
        body {
            margin-top: 3em;
            height: 100vh;
            width: 100vw;
            -Ibkit-background-size: cover;
            -moz-background-size: cover;
            -o-background-size: cover;
            background-size: cover;
        }
        
        a {
            text-decoration: none;
        }
        
        ul {
            list-style-type: none;
        }
        
        #content {
            min-width: 870px;
            margin: 160px 0px 0px 145px;
            padding: 0%;
        }
        
        #content ul {
            background-color: #A8D6FF;
        }
        
        #content ul li {
            font-size: 3em;
            margin-right: 30px;
            display: inline-block;
        }
        
        #content ul li a {
            color: #FFFFFF;
        }
        
        #content ul li a:hover {
            opacity: 0.5;
        }
        
        #project {
            margin-left: 42px;
            margin-right: 7%;
        }
        
        #project-title {
            font-size: 3em;
        }
        
        .sub-header {
            font-size: 1.5em;
        }

        .caption {
            font-size: 13px;
        }

        .col
        {
            margin-left: 1%;
            width: 32%;
            float:left;
        }

        @media screen and (max-width: 870px) {
            body {
                margin-top: 3em;
                height: 100vh;
                width: 870px;
                -Ibkit-background-size: cover;
                -moz-background-size: cover;
                -o-background-size: cover;
                background-size: cover;
            }

            #content {
                width: 870px;
                margin: 144px 0px 0px 0px;
                padding: 0%;
                overflow: hidden;
                white-space: nowrap;
            }
        }
    </style>
</header>

<body>
    <div id="content">
        <ul>
            <li>
                <a href="/">HOME</a>
            </li>
            <li>
                |
            </li>
            <li>
                <a href="/about">ABOUT</a>
            </li>
            <li>
                |
            </li>
            <li>
                <a href="/cs585">CS585</a>
            </li>
        </ul>
        <div id="project">
            <span id="project-title">Assignment 4</span>
            <br>
            <span id="project-description">Justin Chen</span>
            <br>
            <span id="project-description">Team: Chenchen Wang, Ruirong Yang, Enda Peng</span>
            <br>
            <span id="project-description">Date: 10/25/16</span>
            <p>
                <span class="sub-header">Problem Definition</span>
                <br> In this assignment, we are given videos on eels and crabs in a tank. The tank is divided into two areas, an open feeding area and a sheltered area where the animals can hide. Using computer vision techniques, our goal is to segment and track the eels and crabs while in the open feeding area and determine when they are most active in the open feeding area of the tank. Additionally, we were tasked with segmenting sections of the eels' bodies and infer a motion model.
            </p>
            <p>
                <span class="sub-header">Method and Implementation</span>
                <br>
                For my segmentation and tracking implementation, I first scaled down the video by dividing the dimensions of the video in half. Next we implemented a drag and crop region of interest feature so the user can manually select where they want the segmentation and tracking algorithm to focus on. Then I applied a skip frame difference technique to the cropped region of interest. This technique would read in the current frame of the video and then skip over several subsequent frames and read the next frame. I then applied the absdiff() function to the current and next frames to get a grayscale image showing the differences, which I assumed would highlight the movement of the eels between the frames. That difference image was then converted to a binary image. Pixels with a grayscale value of 10 or more were set to white and all others were set to black. This segmented any moving eels from the background.
                <br><br>
                To segment individual eels, that binary difference image was copied to another image and then that image was dilated to form a bokeh. The bokeh image was then search for white blobs. When the search came across a white pixel, it would apply breadth-first search and label those pixels in that blob with a color and their corresponding pixels in the differene image. This created a segmentation image with each potential eel labeled with a different color. That segmentation image was that eroded and then dilated to remove noise and then fed to another function to draw bounding boxes around each eel for tracking.
                <br><br>
                Lastly, the difference image was used to determine the eel activity in the feeding area. I implemented another algorithm that to count the number of white pixels in the region of interest in each frame. The white pixel would was then saved in a vector, which keeps a running count throughout the entire video and plotted a histogram. The x-axis represents the frame number or time and the y-axis represents the activity - the white pixel count which is scaled by 10 to more clearly emphasize the difference between frame activity.
                <br>
            </p>
            <p>
                <span class="sub-header">Experiments</span>
                <br> We experimented with different segmentation techniques and thresholding parameters before achieving our final implmentation. One method that was tried used a reference template of an empty feeding area. This was compared to each frame. If any differences appeared, it was assumed to be an eel. This method did not work very well since the user must first manually identify an empty frame and crop the video manually to do matching. Another method used an additive frame differencing. Two frames were compared and the additional pixels that appeared in the current frame that weren't in the previous frame were assumed to be part of the eel and were added to pixels in the previous frame which were assumed to be part of the eel. One of the issues with this method was that pixels that were assumed to be part of the eel in previous frames were not updated and so remained segmented and were labeled as eels when though the eel was no longer at that position in the frame.
                <br><br>
                For my segmentation and tracking, I tested the following paramters: frame skip rate, binary threshold level, dilation factors. Initially we only used the immediate next frame, which is a skip rate of 0. When the eel moved only a little but, only the pixels closer to the edge of its body could be detected, but the center of the body didn't change enough to be detected. Increasing the skip frame rate allowed enough frames to pass so that we the absdifference() function could detected that the eel had moved. We slowly incremented the this parameter and found that skipping 7 frames was enough to notice the majority of the pixels that belonged to the eel had moved. This helped for capturing the area around the eel so that we could draw a bounding box. As for the binary threshold level, we found that increasing the threshold too high set everything to black and setting the threshold too low captured too much noise. We increamentally adjusted the threshold values and found the following worked the best: thresholding at 10 after the initial frame difference and thresholding after generating the bokeh image at 7, and before getting the white pixels count for eel activity. Similarly, we adjusted the dilation factor to 4 for creating the bokeh image and the errosion factor to 1 and dilation to 7 for removing noise before drawing the bounding boxes.
            </p>
            <p>
                <span class="sub-header">Results</span>
                <br><br>
                <span class="sub-header">Sample Properties Output</span>
                <br>
                <img src="/cs585/a4/img/seg-good-main.png" height="630" width="900">
                <br><br>
                <span class="sub-header">Eels and Crabs Segmentation, Tracking, and Analysis</span>
                <div class="col">
                    Lost tracking
                    <br>
                    <img src="/cs585/a4/img/seg-bad-1.png" height="160" width="360">
                    <br> 
                    Eel occluding each other
                    <br>
                    <img src="/cs585/a4/img/seg-bad-2.png" height="160" width="360">
                    <br>
                    Overestimated bounding box
                    <br>
                    <img src="/cs585/a4/img/seg-bad-3.png" height="160" width="360">
                    <br>
                </div>
                <div class="col">
                    Partial Segmentation of eel body
                    <br>
                    <img src="/cs585/a4/img/seg-partial-1.png" height="160" width="360">
                    <br> 
                    Fragmented eel bodies
                    <br>
                    <img src="/cs585/a4/img/seg-partial-2.png" height="160" width="360">
                    <br>
                    Occluded eels labeled as a single eel
                    <br>
                    <img src="/cs585/a4/img/seg-partial-3.png" height="160" width="360">
                    <br>
                </div>
                <div class="col">
                    Good example 1
                    <br>
                    <img src="/cs585/a4/img/seg-good-1.png" height="160" width="360">
                    <br> 
                    Good example 2
                    <br>
                    <img src="/cs585/a4/img/seg-good-2.png" height="160" width="360">
                    <br>
                    Good example 3
                    <br>
                    <img src="/cs585/a4/img/seg-good-3.png" height="160" width="360">
                    <br>
                </div>
            </p>
            <p>
                <span class="sub-header">Discussion</span>
                <br> 
                <p>
                For this assignment, we only focused on segmenting, tracking, and analyzing the eel. The crab never moved and both the crabs and the background are white, so segmenting the crabs was impractical. Even trying to find the crabs just by looking at the image was difficult. Overall, segmenting the eels was somewhat difficult. The eels would occlude each other or not move very much. Sometimes the eels would swim close to the top of the videoa and would create a reflection. Our segmentation system would confuse the reflection for an actual eel. Another challenge with our segmentation system is that if eels are too close together, the bokeh algorithm labels eels that are close by, but not touching as a single eel. Lighting also caused issues with determining the eel body.
                <br><br>
                As for the analysis, we were only able to determine basic features such as the centroid, orientation, and circularity by using my code from the last assignment. We could not determine undulation pattern without having a perfect segmentation. If we had a perfect segmentation, we could have determined the position of the eel - if it was swimming vertically or horizontally. Then we could take the segmented pixels and transposes the coordinates if it were swimming up or down so that we could get a horizontal representation. Then we could scan across these pixels from the minimum x-coordinate to the max after normalizing the cooridates, shifting all the pixels so that the min is at 0. Then we could find the maximum y-value for each x-value and collect these points. With these values, we now have a potential sine wave. We could then travel these points and detemine the peeks and valleys and calculate the undulation pattern, or try to fit a curve sine wave using something like a neural network. Additionally, because we were unable to obtain a perfect segmentation, we could not determine individual body parts of the eel (head, trunk, tail).
                <br><br>
                As for the activity in the feeding area, we think that counting the white pixels in the region of interest reasonably corresponds to the eels activity during feeding. One clear down side is that when the lighting increases, that could be mistaken for more eel activity. Given more time, would could have further analyzed then activity patterns and determined the peek histogram values as times when there were most activity.
                </p>
            </p>
            <p>
                <span class="sub-header">Conclusions</span>
                <br> Segmenting, tracking, and analysis of objects is difficult even in a reasonably controlled environment. Lighting, occlusion, and color make these tasks very difficult. A stronger system should be flexible enough to adapt to these types of changes without prior knowledge. I believe that a convolutonal neural network segmentation and tracking could increase the fidelity of this system.
            </p>
            <p>
                <span class="sub-header">Credits and Bibliography</span>
                <br> 
                <a target="_blank" href="http://life2coding.blogspot.com/2015/06/cropping-image-using-mouse.html">http://life2coding.blogspot.com/2015/06/cropping-image-using-mouse.html</a><br>
            </p>
        </div>
    </div>
</body>
<footer>
</footer>

</html>
