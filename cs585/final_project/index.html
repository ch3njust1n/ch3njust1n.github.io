<!--
        Justin Chen
        CS585 A1

        Assignment one webpage

        Boston University Computer Science 
        9.13.2016
-->

<!DOCTYPE html>
<html>
<header>
    <meta charset="UTF-8">
    <base href="/">
    <title>Justin Chen - CS585</title>
    <link rel='stylesheet' type='text/css' href='css/normalize.css'>
    <link rel='stylesheet' type='text/css' href='css/style.css'>
    <style>
        html {
            font-family: Avenir Next;
        }
        
        body {
            margin-top: 3em;
            height: 100vh;
            width: 100vw;
            -webkit-background-size: cover;
            -moz-background-size: cover;
            -o-background-size: cover;
            background-size: cover;
        }

        li {
            list-style-type: none;
        }

	a {
	    text-decoration: none;
 	}
        
        #content {
            min-width: 870px;
            margin: 160px 0px 0px 145px;
            padding: 0%;
        }
        
        #nav-bar {
            background-color: #A8D6FF;
        }
        
        #nav-bar li {
            font-size: 3em;
            margin-right: 30px;
            display: inline-block;
        }
        
        #nav-bar li a {
            color: #FFFFFF;
        }
        
        #nav-bar li a:hover {
            opacity: 0.5;
        }
        
        #project {
            margin-left: 42px;
            margin-right: 10%;
        }
        
        #project-title {
            font-size: 3em;
        }

        .sub-header {
            font-size: 1.5em;
            margin-bottom: 1em;
            clear:both;
            font-weight: 400;
        }

        .sub-header-1 {
            font-size: 1.3em;
            margin-bottom: 1em;
            clear:both;
            font-weight: 400;
        }

        .citations {
            margin: 0px;
        }

	.citations li a {
            text-decoration: underline;
        }

        @media screen and (max-width: 870px) {
            body {
                margin-top: 3em;
                height: 100vh;
                width: 870px;
                -webkit-background-size: cover;
                -moz-background-size: cover;
                -o-background-size: cover;
                background-size: cover;
            }

            #content {
                width: 870px;
                margin: 144px 0px 0px 0px;
                padding: 0%;
                overflow: hidden;
                white-space: nowrap;
            }
        }
    </style>
</header>

<body>
    <div id="content">
        <ul id="nav-bar">
            <li>
                <a href="/chenjus">HOME</a>
            </li>
            <li>
                |
            </li>
            <li>
                <a href="/about">ABOUT</a>
            </li>
            <li>
                |
            </li>
            <li>
                <a href="/cs585">CS585</a>
            </li>
        </ul>
        <div id="project">
            <span id="project-title">Transfer Learning with DCGANs</span>
            <br>
            <span id="project-description">Justin Chen</span>
            <br>
            <span id="project-description">Team: Chengchen Wang</span>
            <br>
            <span id="project-description">Date: 12.6.16</span>
            <br>
            <span id="project-description">Proposal: 3</span>
            <p>
                <i>We reformulated our final project so that it could be reasonably completed within a semester.</i>
            </p>
            <div>
                <h3 class="sub-header">1. Problem Definition</h3>
                <div>
                    One of the most expensive aspects of learning algorithm is training time. Depending on the size of your data and depth of your neural network, training can hours, days, over even weeks with no guarantee of success. If the architecture needs to be changed or hyperparameters need to be refined, the model typically needs to be retrained from scratch. The transfer learning training method [3], addresses by using a semantically pretrained model to initialize the weights for learning a new dataset. In 2013, Oquab <i>et al.</i> [5] demonstrated the effectiveness of <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">transfer learning with convolutional neural networks</a> by retraining the classifier. Since then, transfer learning has been widely used as an initialization technique. However, transfer learning has not been used in purely convolutional architectures, which was only recently developed in 2015 by Radford <i>et al.</i> [6] with the introduction of <a href="https://arxiv.org/abs/1511.06434">Deep Convolutional Generative Adversarial Networks</a>. We aim to naively show that transfer learning is also possible with purely convolutional architectures to learn images of dogs by first transferring from a learned model of cat images.
                </div>
            </div>
            <div>
                <h3 class="sub-header">2. Method and Implementation</h3>
                <div>
                    We took several approaches with this project to be able to complete it within a reasonable amount of time. Our first approach was to find a model pretrained DCGAN on ImageNet in TensorFlow. We figured that a model trained on ImageNet would contain very rich features that we could transfer. The only DCGAN implementation we could find was from <a href="https://github.com/Newmu/dcgan_code">Alec Radford's original code</a>, but it was in Theano. We implemented transfer learning in the Theano code, but encountered inherent issues with Theano and using our GPU. We then switched back to using Tensorflow, but could not find any pretrained ImageNet models. Our final code based on <a href="https://github.com/carpedm20/DCGAN-tensorflow">Taehoon Kim's DCGAN</a> implementation with minor modifications for transfer learning.
                    <br>
                    To do transfer learning, the code first checks for any pretrained models if the user specifies the number of layers to transfer. After loading the variables from the pretrained model, our implementation keeps the desired number of layers to transfer and reinitializes the remaining layers with random values. Reinitializing the last <i>k</i> layers is the standard approach for transferring when the network uses fully-connected layers. However, the entire model could also be transferred and used to initialize the dog model, but we chose to start with the standard technique for all our experiments to observe the results since this has never been performed on DCGAN's which are purely convolutional architectures.
            </div>
            <div>
                <h3 class="sub-header">3. Experiments</h3>
                <div>
                    All experiments and neural networks were run using a Nvidia GTX 1070 with an i7 core CPU, and 32 gigabytes of RAM.
                </div>
                <div>
                    <h3 class="sub-header-1">3.1 Preprocessing</h3>
                    <div>
                        After manually inspecting the collected cat and dog images, we found that there was a lot of noise. Some images contained cat figurines, cartoons, and a lot of background noise and other distracting objects. To address this, we cleaned our dataset by running it through Google's Inception network [7] and only kept images if Inception's best classification score was a cat or dog. This reduced our cat images from 45,721 images to 14,478 images, and 57,941 images of dogs to 8,979 images. Simultaneously, we converted all images from grayscale to RGB. After filtering through Inception, we resized all images to 128 x 128 pixels. We then trained DCGAN from scratch on our cat images.
                        <br>
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/inception.png">
                    </div>
                </div>
                <div>
                    <h3 class="sub-header-1">3.2 24 epochs</h3>
                    To test our initial transfer learning implementation, we trained a cat model for 24 epochs with 11,230 cat images from ImageNet and then transferred the first three out of four layers to train a dog model with 17,095 images from Stony Brook's Im2Text dataset. The choice in datasets for this preliminary test were arbitrary.
                    <br>
                    <div>
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/24-epochs_cat2dog.jpg">
                    </div>
                </div>
                <div>
                    <h3 class="sub-header-1">3.3 35 epochs</h3>
                    For our second experiment, we trained the cat model for longer for 35 epochs, which took about two hours. The model seemed to learn edges, colors, and colors of cats. With limited data, the DCGAN had difficulty learning the amorphous shape of cat bodies. However, the one structure that remained consistent throughout all the images was the shape of cat faces. The model learned the shape and position of cat ears on the face and learned to differentiate background from foreground. This can be seen by the fact that the generative network generates solid areas around the cat's face. By the end of training, the model seemed to be learning the eyes, nose, and mouth of cats.
                    <br>
                    <div>
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/35-epochs_cat2dog.jpg">
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/35-epochs_cat2dog_features.jpg">
                    </div>
                </div>
                <div>
                    <h3 class="sub-header-1">3.4 325 Epochs</h3>
                    We then continued training the cat model for another 290 epochs, which took an additional twelve hours. Interestingly, but not surprisingly, DCGAN began repeating generated images (red circles). The repeated images diverged and lost any resemblence to cats or the learned features it had around 35 epochs. This collapse in pattern is indication of underfitting in DCGANs. Bias is generally caused by insufficient amount of data, which only covers a limited portion of the space. With limited data and excessive training, the model tries to generalize all the shapes it learns across the images instead of the intended patterns that it should capture across the training data.
                    <br>
                    <div>
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/325-epochs_cat2dog.jpg">
                    </div>
                </div>
                <div>
                    <h3 class="sub-header-1">3.5 Data Augmentation</h3>
                    Given the lack of data, we augmented our cat dataset by applying a sequence of twenty random transformations. The set of transformations we chose were horizontal and vertical mirroring, Gaussian blur, contrast, brightness, unsharp mask, rank filter, median filter, min filter, max, filter, and mode filter. we only augmented the cat dataset resulting in a total of 291,728 images. We then trained the model again, but killed it shortly after a few epochs after noticing poor results from the images outputted every 100 epochs.
                    <br>
                    <img max-width=95% max-height=95% src="/cs585/final_project/img/augmented_cats.jpg">
                    <br>
                    <h3 class="sub-header-1">Results from training on augmented data:</h3>
                    <br>
                    <div>
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/bad_aug_1.jpg">
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/bad_aug_2.jpg">
                        <img max-width=95% max-height=95% src="/cs585/final_project/img/bad_aug_3.jpg">
                    </div>
                </div>
                <div>
                    <h3 class="sub-header-1">3.6 LSUN 20 Objects Cat and Dog Images</h3>
                    Finally, we attempted to train DCGAN on random samples from the LSUN (Large-scale Scene Understanding) 20 objects dataset with cat and dog images. We randomly sampled 600,000 cat images and 100,000 dog images and filtered them with GoogleNet. After filtering, we were left with only 299,500 cat images. Due to lack of time and computing resources, we could not download the LSUN dog dataset, which is 145 GB. However, if we had more time, we had planned on sampling 100,000 images for transfer learning dogs. 
                    <div>
			<h3 class="sub-header-1">Final results from cat model:</h3>
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_1.jpg">
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_2.jpg">
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_3.jpg">
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_4.jpg">
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_5.jpg">
			<img max-width=95% max-height=95% src="/cs585/final_project/img/300k_6.jpg">
		    </div>
		</div>
            </div>
            <div>
                <h3 class="sub-header">4. Future Work</h3>
		We hope to continue work on transfer learning with DCGANs. The research we have done so far has only tried naive implements and only on the standard DCGAN. Since the inception of DCGAN in the 2015, a myriad of DCGAN-based architectures have been developed by the deep learning community. One thing we are curious about is if it is possible to transfer the convolutional architecture from any arbitrary convolutional neural network and repurpose it with adversarial training. Work can also be done with the performance of transfer learning with some of the newer variations of DCGAN like [11]. Training DCGANs with less data stably [9] or with little data [10] are other unexplored areas of research.
     	    </div>
            <div>
                <h3 class="sub-header">5. Conclusion</h3>
                In conclusion, DCGAN requires at least two orders of magnitude more data - at least on the order of millions of images to be able to learn the amorphous shape of cats. However, with only about 300,000 images, DCGAN can learn cat faces because these structures are very consistent throughout each image in the dataset. From our preliminary test, we also conclude that transfer learning in DCGANs is possibe and could potentially produce high quality images if equip with more data.
	    </div>
            <div>
                <h3 class="sub-header">6. References</h3>
                <div class="citations">
                    <li>[1] Deng, Jia, et al. <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">"Imagenet: A large-scale hierarchical image database."</a> Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.
                    </li>
                    <li>[2] Elson, Jeremy, et al. <a href="https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/">"Asirra: a CAPTCHA that exploits interest-aligned manual image categorization."</a> ACM Conference on Computer and Communications Security. Vol. 7. 2007.
                    </li>
                    <li>[3] Karpathy, A. (2016, January 3). Transfer Learning. Retrieved December 4, 2016, from <a href="http://cs231n.github.io/transfer-learning/">http://cs231n.github.io/transfer-learning/</a>
                    </li>
                    <li>[4] Ordonez, Vicente, Girish Kulkarni, and Tamara L. Berg. <a href="http://vision.cs.stonybrook.edu/~vicente/sbucaptions/">"Im2text: Describing images using 1 million captioned photographs."</a> Advances in Neural Information Processing Systems. 2011.
                    </li>
                    <li>[5] Oquab, Maxime, et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">"Learning and transferring mid-level image representations using convolutional neural networks."</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.
                    </li>
                    <li>[6] Radford, Alec, Luke Metz, and Soumith Chintala. <a href="https://arxiv.org/abs/1511.06434v2">"Unsupervised representation learning with deep convolutional generative adversarial networks."</a> arXiv preprint arXiv:1511.06434 (2015).
                    </li>
                    <li>[7] Szegedy, Christian, et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html">"Going deeper with convolutions."</a> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.
                    </li>
                    <li>[8] Yu, Fisher, et al. <a href="http://www.yf.io/p/lsun">"Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop."</a> arXiv preprint arXiv:1506.03365
                    </li>
		    <li> [9] Metz, Luke, et al. <a href="https://arxiv.org/abs/1611.02163">"Unrolled Generative Adversarial Networks."</a> arXiv preprint arXiv:1611.02163 (2016).
                    </li>
                    <li>[10] Lou, Xinghua, et al. <a href="http://papers.nips.cc/paper/6071-generative-shape-models-joint-text-recognition-and-segmentation-with-very-little-training-data">"Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data."</a> Advances In Neural Information Processing Systems. 2016.
                    </li>
                    <li>[11] Reed, Scott, et al. <a href="https://arxiv.org/abs/1605.05396">"Generative adversarial text to image synthesis."</a> arXiv preprint arXiv:1605.05396 (2016).
                    </li>
                </div>
            </div>
        </div>
    </div>
</body>
<footer>
</footer>

</html>
